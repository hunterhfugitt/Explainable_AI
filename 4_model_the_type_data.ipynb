{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtgsdk import Card\n",
    "from mtgsdk import Set\n",
    "from mtgsdk import Type\n",
    "from mtgsdk import Supertype\n",
    "from mtgsdk import Subtype\n",
    "from mtgsdk import Changelog\n",
    "import os\n",
    "import nltk\n",
    "import regex as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import math\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy import spatial\n",
    "import sklearn \n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import hdbscan\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "import sys\n",
    "from random import sample\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\\\Users\\\\hunte\\\\coding projects')\n",
    "all_data = json.load(open('cards_object.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _addtokey_(relevant_key,relevant_dict,relevant_element):\n",
    "    string_to_add = ''\n",
    "    header = f\" = {relevant_key} = \"\n",
    "    pattern = r'([^\\w\\s])'\n",
    "    replacement = r' \\1 '\n",
    "    for each in relevant_dict[relevant_key]:\n",
    "        if(each == relevant_element):\n",
    "            subheader = re.sub(pattern, replacement, each)\n",
    "            subheader = f\" = = {subheader} = = \"\n",
    "            string_to_add = string_to_add + subheader + '\\n\\n'\n",
    "            try:\n",
    "                text = relevant_dict[relevant_key][each]\n",
    "            except:\n",
    "                text = 'null'\n",
    "            if(type(text) is list):\n",
    "                delete_these = []\n",
    "                for count2,element in enumerate(text):\n",
    "                    if(type(element) is dict):\n",
    "                        for key_2 in element:\n",
    "                            sub_sub_header = re.sub(pattern, replacement, key_2)\n",
    "                            sub_sub_header = f\" = = = {sub_sub_header} = = = \"\n",
    "                            string_to_add = string_to_add + sub_sub_header + '\\n\\n'\n",
    "                            try:\n",
    "                                #print(element[key_2])\n",
    "                                #print(re.sub(pattern, replacement, element[key_2]))\n",
    "                                string_to_add = string_to_add + (re.sub(pattern, replacement, element[key_2].replace(f\"{relevant_key}\", \"self\")) + '\\n\\n')\n",
    "                            except:\n",
    "                                pass\n",
    "                        delete_these.append(count2)\n",
    "                for count3,counted_value in enumerate(delete_these):\n",
    "                    text.pop(counted_value - count3)\n",
    "                text = ' '.join(text)\n",
    "            try:\n",
    "                text = re.sub(pattern, replacement, text.replace(f\"{relevant_key}\", \"self\"))\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                string_to_add = string_to_add + (re.sub(pattern, replacement, text) + '\\n\\n')\n",
    "            except:\n",
    "                pass    \n",
    "    return(string_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = []\n",
    "\n",
    "for example_type in all_data[list(all_data.keys())[0]]:\n",
    "    list_features.append(example_type)\n",
    "\n",
    "Big_list = []\n",
    "\n",
    "for feature in list_features:\n",
    "    new_dict = {}\n",
    "    for each in all_data:\n",
    "        new_dict[each] = _addtokey_(each,all_data,feature)\n",
    "    Big_list.append(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\\\Users\\\\hunte\\\\coding projects\\\\cleaned_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count,feature in enumerate(list_features):\n",
    "    with open(f\"cleaned_up_{feature}_data.json\", \"w\") as f:\n",
    "        json.dump(Big_list[count], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features.pickle', 'wb') as f:\n",
    "    pickle.dump(list_features, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
